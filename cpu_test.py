# -*- coding: utf-8 -*-
import os
import torch
from PIL import Image
import copy
import numpy as np
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX

print("ğŸš€ Running SD-VLM pipeline with built-in Depth-Anything v2 ...")

# ========== 1ï¸âƒ£ è£ç½®è¨­å®š ==========
device = torch.device("cpu")   # æ”¹æˆ "cuda" è‹¥å¯ç”¨ GPU
torch.set_default_device(device)
torch.set_num_threads(8)
print(f"âœ… Using device: {device}")

# ========== 2ï¸âƒ£ è¼‰å…¥ SD-VLM ä¸»æ¨¡å‹ ==========
model_path = "/home/itris3/SD-VLM/SD-VLM-7B"
tokenizer, model, image_processor, context_len = load_pretrained_model(
    model_path=model_path,
    model_base=None,
    model_name=get_model_name_from_path(model_path)
)
model.to(device)
print("âœ… SD-VLM model loaded")
# âœ… å¾¹åº•è½‰æˆ float32ï¼Œé¿å… Float / Half æ··ç”¨éŒ¯èª¤
for name, param in model.named_parameters():
    if param.dtype == torch.float16 or param.dtype == torch.bfloat16:
        param.data = param.data.float()
        if param.grad is not None:
            param.grad = param.grad.float()
print("âœ… å…¨æ¨¡å‹æ¬Šé‡å·²è½‰ç‚º float32 (CPU safe)")


# ========== 3ï¸âƒ£ è¼‰å…¥å…§å»º Depth Anything ==========
# ========== 3ï¸âƒ£ è¼‰å…¥å…§å»º Depth Anything ==========
print("âš™ï¸ Loading built-in Depth Anything v2 (from local checkpoint)...")
# âœ… åˆå§‹åŒ– DepthAnything æ¨¡å‹ï¼ˆæ‰‹å‹•è¼‰å…¥æ¬Šé‡ï¼‰
from llava.model.depth.depth_anything_v2.dpt import DepthAnythingV2

# ğŸ”§ æŒ‡å‘ä½ æœ¬åœ°çš„æ¨¡å‹æ¬Šé‡
depth_ckpt_path = "/home/itris3/SD-VLM/llava/model/depth/depth_anything_v2/depth_anything_v2_vitl.pth"

# âœ… å»ºç«‹ DepthAnythingV2 æ¨¡å‹ï¼ˆä¸å‚³å…¥ model_typeï¼‰
vit_depth = DepthAnythingV2()  # å®˜æ–¹ dpt.py ä¸éœ€ model_type

# âœ… è¼‰å…¥æœ¬åœ°æ¬Šé‡
checkpoint = torch.load(depth_ckpt_path, map_location=device)
# æœ‰äº›ç‰ˆæœ¬çš„ checkpoint æ˜¯ {'model': state_dict} çµæ§‹ï¼Œæ‰€ä»¥åšå€‹é˜²å‘†ï¼š
if "model" in checkpoint:
    vit_depth.load_state_dict(checkpoint["model"])
else:
    vit_depth.load_state_dict(checkpoint)

vit_depth.to(device)
vit_depth.eval()

# âœ… é™„åŠ åˆ° SD-VLM æ¨¡å‹ä¸­
model.vit_depth = vit_depth
print("âœ… Attached built-in depth module to model.vit_depth (from local .pth)")


# ========== 4ï¸âƒ£ ç¢ºä¿ vision_tower æ­£å¸¸ ==========
vision_tower = model.get_model().get_vision_tower()
if hasattr(vision_tower, "load_model") and not getattr(vision_tower, "is_loaded", False):
    print("âš™ï¸ Loading CLIP vision tower ...")
    vision_tower.load_model(device_map=None)
    print("âœ… Vision tower ready.")
image_processor = vision_tower.image_processor

# ========== 5ï¸âƒ£ æº–å‚™è¼¸å…¥ ==========
prompt = "How far is the chair in this image?"
image_folder = "/home/itris3/SD-VLM/images"
image_file = "test.jpg"
image_path = os.path.join(image_folder, image_file)
assert os.path.exists(image_path), f"âŒ æ‰¾ä¸åˆ°åœ–ç‰‡: {image_path}"

image = Image.open(image_path).convert("RGB")


ori_img = copy.deepcopy(image)
input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt").unsqueeze(0).to(device)

# ========== 6ï¸âƒ£ åœ–åƒè™•ç† ==========
image_tensor = process_images([image], image_processor, model.config)[0].unsqueeze(0).to(device)
if image_tensor.dtype != torch.float32:
    image_tensor = image_tensor.float()

# ========== 7ï¸âƒ£ æ¨è«– ==========
print("ğŸ§  Running SD-VLM + Depth-Anything inference ... (CPU may take 3â€“8 mins)")
# âœ… å°‡ PIL Image è½‰æˆ Tensorï¼Œç¬¦åˆ resize_depth() é æœŸè¼¸å…¥
from torchvision import transforms

to_tensor = transforms.ToTensor()
ori_img_tensor = to_tensor(ori_img).to(device).float()

with torch.inference_mode():
    output_ids = model.generate(
        input_ids,
        images=image_tensor,
        image_sizes=[image.size],
        do_sample=False,
        temperature=0.2,
        num_beams=1,
        ori_imgs=[ori_img_tensor],  # âœ… æ”¹æˆ tensor
        max_new_tokens=64,
        use_cache=True,
    )
# ========== 8ï¸âƒ£ è¼¸å‡º ==========
response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
print("ğŸŸ¢ Model response:", response)
